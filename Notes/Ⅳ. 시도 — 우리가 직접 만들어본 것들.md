# Ⅳ. 시도 — 우리가 직접 만들어본 것들

## 1️⃣ 처음엔 ‘지도 위의 챗봇’이었다
- 우리 팀의 첫 구상은 단순했다.  
  “지도 위에서 재난이 발생하면, 그 위치를 기준으로 행동을 알려주는 챗봇.”  
- 하지만 막상 구현하려 하니 **‘지도만 있다고 행동을 판단할 수는 없다’**는 걸 깨달았다.  
- 그래서 “지도”보다 “판단의 흐름”에 집중하기로 했다.  
  → **‘위치 → 데이터 → 상황 판단 → 행동 제시’**라는 프로세스를 정립했다.

## 2️⃣ 우리가 만든 구조
- 해커톤 하루 동안, 우리는 최소한의 기능이 돌아가는 형태를 목표로 했다.  
  전체 구조는 이렇게 잡았다:

```python
사용자 입력
↓
오케스트레이터 (LLM + 규칙)
↓
프로파일 에이전트 (사용자 상황 분석)
↓
플래닝 에이전트 (행동 시나리오 설계)
↓
애널리스트 에이전트 (데이터 기반 검증)
↓
LLM 합성 (행동 지침 생성)
↓
결과 시각화 (지도/텍스트/경로)
```


- LLM은 문장을 만드는 게 아니라, **“판단 루프를 돌리는 역할”**로 설계했다.  
- 각 에이전트는 서로 다른 관점을 가졌고,  
  이들이 협업하면서 하나의 “행동 가이드”를 만들어냈다.

## 3️⃣ 기술적으로 시도한 것들
- **LangChain 기반 Multi-Agent 구조**
  - 오케스트레이터(LLM)가 플래닝/애널리스트/프로파일 에이전트를 조율.  
  - 각 에이전트가 메모리에 자신의 결과를 남기고, 다른 에이전트가 그걸 참조.  
  - 결과적으로, 단일 LLM이 아니라 **협업형 의사결정 구조**를 구현했다.
- **RAG 문헌 검색**
  - 「자연재해대책법」, 「재난 및 안전관리 기본법」, 국민재난안전포털 행동요령을 벡터화.  
  - 실제 정책과 행동 매뉴얼을 근거로 행동을 제시하도록 설계.  
- **프롬프트 검증 훅(Prompt Validation Hook)**
  - LLM이 비논리적 판단을 내릴 경우, 다시 한 번 점검하는 장치.  
  - “이 행동은 법적 기준과 일치합니까?” 같은 후속 질의 자동 삽입.

## 4️⃣ 실제 시연 예시
- 예를 들어, 사용자가 “지금 지진이 발생했어요. 밖으로 나가야 하나요?”라고 입력하면:
  1. **프로파일 에이전트**가 사용자의 위치(3층 실내)·시간대·GPS 신호를 확인.  
  2. **애널리스트 에이전트**가 내진 구조 여부와 주변 개방 공간을 검색.  
  3. **플래닝 에이전트**가 두 정보를 종합해 “즉시 대피 금지 → 책상 밑 대피 후 진동 종료 시 이동”이라는 결론 도출.  
  4. **LLM 합성 레이어**가 행동 문장과 근거를 함께 출력.
- 결과:
  > “현재 건물 3층, 내진 구조물. 진동 중 이동은 위험합니다.  
  > 탁자 아래로 몸을 숙이고, 진동이 멈춘 뒤 엘리베이터를 피해서 계단으로 내려가세요.”

- 즉, 단순 지시가 아니라 **맥락+근거+행동 순서**를 동시에 제시했다.

## 5️⃣ 데이터 없이도 판단할 수 있게
- 인터넷 연결이 끊겨도 작동할 수 있도록,  
  **온디바이스 LLM 구조(로컬 판단형)**의 방향성도 일부 설계했다.  
- 이번 해커톤에선 구현까지는 못 갔지만,  
  “데이터가 없어도 판단은 가능해야 한다”는 원칙은 끝까지 유지했다.

## 6️⃣ 실패했던 시도들
- **실시간 API 불안정**  
  - 기상청 API가 초당 호출 제한이 있어서, 데이터 동기화가 자주 끊겼다.  
- **LLM의 맥락 누락**  
  - 긴 대화 중 메모리가 사라져, 앞선 행동 판단을 잊는 경우 발생.  
- **시간 부족**  
  - 하루 안에 완전한 오케스트레이션을 구현하기엔 너무 벅찼다.  
  - 하지만 ‘구조를 완성했다’는 점 자체가 큰 의미였다.

## 7️⃣ 그럼에도 남은 것
- 비록 완전한 시스템은 아니었지만,  
  이번 시도를 통해 **AI가 실제 재난 문맥에서 ‘판단’ 역할을 할 수 있다**는 가능성을 직접 확인했다.  
- 그 한 줄의 코드, 한 줄의 응답이  
  “사람의 생존 행동을 바꾸는 단서”가 될 수 있다는 걸 경험했다.
