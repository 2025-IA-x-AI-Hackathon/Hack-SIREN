# Ⅵ. 배움 — 실패 속에서 얻은 통찰

## 1️⃣ ‘정답’이 아니라 ‘판단’을 만들어야 했다
- 처음엔 우리가 ‘정답을 알려주는 시스템’을 만들고 있다고 생각했다.  
  하지만 실제로 부딪혀보니,  
  재난 상황에는 정답이 존재하지 않는다는 걸 뼈저리게 느꼈다.  
- 상황은 매 순간 변하고,  
  “정답”은 그때그때의 조건에 따라 달라진다.  
- 그래서 방향을 바꿨다.  
  → **“정답형 AI”가 아니라 “판단형 AI”**,  
  즉 순간의 판단 근거를 만들어주는 시스템으로.

## 2️⃣ AI에게도 ‘근거’가 필요하다는 걸 배웠다
- LLM은 생각보다 말을 잘하지만,  
  ‘왜 그렇게 판단했는가’에 대한 설명은 종종 비어 있었다.  
- 그래서 우리는 행동 지침을 만들 때마다  
  **“근거를 제시하도록 하는 프롬프트 구조”**를 추가했다.  
  → “이 행동의 이유는 무엇입니까?”  
  → “법령 또는 매뉴얼 중 어떤 기준에 근거합니까?”  
- 이렇게 되자 AI의 응답이 훨씬 명료해졌고,  
  “AI의 판단을 신뢰하게 만드는 건 결국 **투명한 사고 과정**”이라는 걸 깨달았다.

## 3️⃣ ‘데이터보다 맥락’이 중요했다
- 초반엔 데이터를 많이 붙이면 더 똑똑해질 거라 믿었다.  
  하지만 오히려 데이터가 많아질수록 AI는 더 혼란스러워졌다.  
- 결국 중요한 건 데이터의 양이 아니라  
  **“지금 이 상황에서 필요한 데이터가 무엇인가”**였다.  
- 이걸 깨닫고 나서, 우리는 모든 모듈의 기준을 바꿨다.  
  → **필요한 데이터만 불러오기.**  
  → **AI가 스스로 정보의 우선순위를 정하도록 설계.**

## 4️⃣ AI의 윤리는 기술보다 앞서야 한다
- 테스트 중, AI가 잘못된 행동지침을 내린 적이 있었다.  
  예를 들어, “지진 중 밖으로 나가세요.”  
  — 사실은 가장 위험한 행동이었다.  
- 이때 우리는 두 가지를 깨달았다.  
  1. **AI의 판단이 곧 생명과 연결될 수 있다.**  
  2. **AI는 ‘지식’보다 ‘책임’을 먼저 설계해야 한다.**  
- 그래서 AI가 행동 지침을 생성하기 전에  
  항상 한 줄의 검증 질문을 추가했다.  
  > “이 행동은 인간의 안전을 최우선으로 보장합니까?”

## 5️⃣ ‘사람’이 결국 핵심이었다
- AI, 데이터, 구조, 코딩…  
  모든 게 복잡했지만, 그 모든 것의 중심에는 결국 **사람**이 있었다.  
- 아무리 정교한 시스템도,  
  실제로 그걸 믿고 따르는 사람이 없다면 의미가 없었다.  
- 그래서 우리는 기술적 완성보다  
  **“사용자가 두려움 속에서도 믿고 따를 수 있는 언어”**를 찾는 게 더 중요하다는 걸 배웠다.

## 6️⃣ 우리가 남긴 교훈
> “AI를 만드는 게 목적이 아니라,  
> AI를 통해 인간의 판단을 회복시키는 게 목적이었다.”

- 이 프로젝트는 실패의 연속이었지만,  
  그 안에서 우리가 얻은 건 **‘기술의 본질은 인간을 돕는 것’**이라는 확신이었다.  
- 재난 속의 AI는 ‘정답을 아는 존재’가 아니라,  
  **‘혼란 속에서도 침착하게 곁에 서 있는 존재’**여야 한다는 걸 깨달았다.  
- 그 통찰이, 이후 우리가 바라보는 **‘Sense.zip’의 철학**이 됐다.
