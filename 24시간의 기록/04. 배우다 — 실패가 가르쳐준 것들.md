# 04. 배우다 — 실패가 가르쳐준 것들

## 1. '정답'이 아니라 '판단'을 만들어야 했다

처음에는 정답을 알려주는 시스템을 만들고 있다고 생각했습니다. 그러나 실제로 부딪혀보니, 재난 상황에는 정답이 존재하지 않는다는 것을 뼈저리게 느꼈습니다.

상황은 매 순간 변하고, 정답은 그때그때의 조건에 따라 달라집니다. 그래서 방향을 바꾸었습니다. 정답형 AI가 아니라 판단형 AI, 즉 순간의 판단 근거를 제공하는 시스템으로 전환했습니다.

## 2. AI에게도 '근거'가 필요하다는 것을 배웠다

LLM은 생각보다 말을 잘하지만, 왜 그렇게 판단했는가에 대한 설명은 종종 부족했습니다. 그래서 행동 지침을 만들 때마다 근거를 제시하도록 하는 프롬프트 구조를 추가했습니다.

"이 행동의 이유는 무엇입니까?", "법령 또는 매뉴얼 중 어떤 기준에 근거합니까?"와 같은 질문을 포함시켰습니다. 이렇게 하자 AI의 응답이 훨씬 명료해졌고, AI의 판단을 신뢰하게 만드는 것은 결국 투명한 사고 과정이라는 것을 깨달았습니다.

## 3. '데이터보다 맥락'이 중요했다

초반에는 데이터를 많이 붙이면 더 똑똑해질 거라 믿었습니다. 그러나 오히려 데이터가 많아질수록 AI는 더 혼란스러워했습니다.

결국 중요한 것은 데이터의 양이 아니라 지금 이 상황에서 필요한 데이터가 무엇인가였습니다. 이를 깨닫고 난 후, 우리는 모든 모듈의 기준을 바꾸었습니다. 필요한 데이터만 불러오고, AI가 스스로 정보의 우선순위를 정하도록 설계했습니다.

## 4. AI의 윤리는 기술보다 앞서야 한다

테스트 중, AI가 잘못된 행동지침을 내린 적이 있었습니다. 예를 들어 "지진 중 밖으로 나가세요"라는 지침은 실제로는 가장 위험한 행동일 수 있습니다.

이때 우리는 두 가지를 깨달았습니다. 첫째, AI의 판단이 곧 생명과 연결될 수 있다는 것입니다. 둘째, AI는 지식보다 책임을 먼저 설계해야 한다는 것입니다.

그래서 AI가 행동 지침을 생성하기 전에 항상 "이 행동은 인간의 안전을 최우선으로 보장합니까?"라는 검증 질문을 추가했습니다.

## 5. '사람'이 결국 핵심이었다

AI, 데이터, 구조, 코딩 등 모든 것이 복잡했지만, 그 모든 것의 중심에는 결국 사람이 있었습니다.

아무리 정교한 시스템이라도 실제로 그것을 믿고 따르는 사람이 없다면 의미가 없었습니다. 그래서 우리는 기술적 완성보다 사용자가 두려움 속에서도 믿고 따를 수 있는 언어를 찾는 것이 더 중요하다는 것을 배웠습니다.

## 6. 이론과 실제의 간극

논문과 문서에서는 간단해 보이는 것들이 실제 구현에서는 훨씬 복잡했습니다. 작동하는 것과 빠르게 작동하는 것은 완전히 다른 문제였습니다.

실패한 시도들이 오히려 더 나은 설계로 이끌었습니다. 에러가 가르쳐준 것이 성공보다 많았습니다.

## 7. 우리가 남긴 교훈

AI를 만드는 것이 목적이 아니라, AI를 통해 인간의 판단을 회복시키는 것이 목적이었습니다.

이 프로젝트는 실패의 연속이었지만, 그 안에서 우리가 얻은 것은 기술의 본질은 인간을 돕는 것이라는 확신이었습니다. 재난 속의 AI는 정답을 아는 존재가 아니라, 혼란 속에서도 침착하게 곁에 서 있는 존재여야 한다는 것을 깨달았습니다.

그 통찰이 이후 우리가 바라보는 Sense의 철학이 되었습니다.

