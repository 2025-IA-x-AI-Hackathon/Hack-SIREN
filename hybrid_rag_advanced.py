# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import os
from typing import List, Dict, Optional
from dotenv import load_dotenv
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# Neo4j
from neo4j import GraphDatabase

# Chroma
import chromadb
from chromadb.config import Settings

# Gemini API
from google import genai
from google.genai import types

load_dotenv()
print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")


# Neo4j ì—°ê²° ì„¤ì •
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "password")

neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

def get_neo4j_session():
    return neo4j_driver.session()

# Neo4j ì—°ê²° í™•ì¸
try:
    with get_neo4j_session() as session:
        result = session.run("RETURN 1 as test")
        print(f"âœ“ Neo4j ì—°ê²° ì„±ê³µ: {NEO4J_URI}")
except Exception as e:
    print(f"âœ— Neo4j ì—°ê²° ì‹¤íŒ¨: {e}")


# Chroma ë²¡í„° DB ì—°ê²° (Gemini ì„ë² ë”© ì‚¬ìš©)
CHROMA_PERSIST_DIR = "data/chroma"
DOCS_DIR = "docs"

chroma_client = chromadb.PersistentClient(
    path=CHROMA_PERSIST_DIR,
    settings=Settings(anonymized_telemetry=False)
)

# Gemini ì„ë² ë”© í•¨ìˆ˜ëŠ” Cell 7ì—ì„œ ì´ˆê¸°í™” í›„ ì‚¬ìš©
# ì„ì‹œë¡œ Noneìœ¼ë¡œ ì„¤ì • (Cell 7 ì‹¤í–‰ í›„ ì—…ë°ì´íŠ¸ í•„ìš”)
gemini_embedding_fn = None

# ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
try:
    chroma_collection = chroma_client.get_collection("disaster_docs")
    print(f"âœ“ Chroma ì»¬ë ‰ì…˜ ë¡œë“œ ì„±ê³µ: {chroma_collection.count()} ë¬¸ì„œ")
except Exception as e:
    print(f"âœ— Chroma ì»¬ë ‰ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ë° ë¬¸ì„œ ì ì¬ ì¤‘...")
    
    # Gemini ì„ë² ë”© í•¨ìˆ˜ê°€ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì„ì‹œ ìƒì„±
    if gemini_embedding_fn is None:
        print("âš  Gemini ì„ë² ë”© í•¨ìˆ˜ê°€ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   Cell 7ì„ ë¨¼ì € ì‹¤í–‰í•˜ê±°ë‚˜, ì•„ë˜ ì½”ë“œë¡œ ì„ì‹œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        # ì„ì‹œ ì´ˆê¸°í™”
        GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
        if GOOGLE_API_KEY:
            temp_client = genai.Client(api_key=GOOGLE_API_KEY)
            class GeminiEmbeddingFunction:
                def __init__(self, client, model="gemini-embedding-001"):
                    self.client = client
                    self.model = model
                def __call__(self, input_texts):
                    result = self.client.models.embed_content(
                        model=self.model,
                        contents=input_texts,
                        config=types.EmbedContentConfig(
                            output_dimensionality=384  # ê¸°ì¡´ ì»¬ë ‰ì…˜ê³¼ í˜¸í™˜ë˜ë„ë¡ 384ì°¨ì› ì‚¬ìš©
                        )
                    )
                    embeddings = []
                    for embedding in result.embeddings:
                        if hasattr(embedding, 'values'):
                            embeddings.append(list(embedding.values))
                        elif isinstance(embedding, list):
                            embeddings.append(embedding)
                        elif hasattr(embedding, '__iter__') and not isinstance(embedding, str):
                            embeddings.append(list(embedding))
                        else:
                            embeddings.append([float(embedding)])
                    return embeddings
            gemini_embedding_fn = GeminiEmbeddingFunction(temp_client)
            print("  âœ“ ì„ì‹œ Gemini ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (Gemini ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©)
    chroma_collection = chroma_client.create_collection(
        name="disaster_docs",
        embedding_function=gemini_embedding_fn,
        metadata={"description": "ì¬ë‚œ í–‰ë™ìš”ë ¹ ë¬¸ì„œ (Gemini ì„ë² ë”© ì‚¬ìš©)"}
    )
    
    # docs í´ë”ì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸° ë° ì²­í‚¹ ê¸°ë°˜ ì ì¬
    import glob
    import re
    
    def chunk_markdown(content: str, chunk_size: int = 500, overlap: int = 100) -> List[Dict[str, str]]:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì²­í‚¹í•©ë‹ˆë‹¤.
        
        Args:
            content: ë§ˆí¬ë‹¤ìš´ ë‚´ìš©
            chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
            overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ (ë¬¸ì ìˆ˜)
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ê° ì²­í¬ëŠ” {"text": ..., "section": ...} í˜•íƒœ)
        """
        chunks = []
        
        # í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„í• 
        sections = re.split(r'(^#+\s+.+$)', content, flags=re.MULTILINE)
        
        current_section = "ì „ì²´"
        current_text = ""
        
        for i, section in enumerate(sections):
            if not section.strip():
                continue
            
            # í—¤ë”ì¸ ê²½ìš°
            if re.match(r'^#+\s+.+$', section.strip()):
                # ì´ì „ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ì²­í¬ë¡œ ì €ì¥
                if current_text.strip():
                    # í˜„ì¬ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                    text_chunks = split_text(current_text, chunk_size, overlap)
                    for j, chunk in enumerate(text_chunks):
                        chunks.append({
                            "text": chunk,
                            "section": current_section
                        })
                
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                current_section = section.strip()
                current_text = section + "\n\n"
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ê°€
                current_text += section + "\n\n"
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if current_text.strip():
            text_chunks = split_text(current_text, chunk_size, overlap)
            for j, chunk in enumerate(text_chunks):
                chunks.append({
                    "text": chunk,
                    "section": current_section
                })
        
        return chunks
    
    def split_text(text: str, chunk_size: int, overlap: int) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ê³ ì • í¬ê¸°ë¡œ ë¶„í•  (ê²¹ì¹¨ í¬í•¨)
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸°
            overlap: ê²¹ì¹˜ëŠ” í¬ê¸°
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° (ê°€ëŠ¥í•œ ê²½ìš°)
            if end < len(text):
                # ë¬¸ì¥ ë ë§ˆì»¤ ì°¾ê¸°
                sentence_end = max(
                    text.rfind('ã€‚', start, end),
                    text.rfind('.', start, end),
                    text.rfind('\n', start, end)
                )
                
                if sentence_end > start + chunk_size // 2:  # ë„ˆë¬´ ì•ìª½ì´ ì•„ë‹ˆë©´
                    end = sentence_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap  # ê²¹ì¹¨ ì„¤ì •
        
        return chunks
    
    doc_files = glob.glob(f"{DOCS_DIR}/*.md")
    print(f"\nì°¾ì€ ë¬¸ì„œ íŒŒì¼: {len(doc_files)}ê°œ")
    
    documents = []
    ids = []
    metadatas = []
    
    chunk_counter = 0
    for doc_file in doc_files:
        try:
            with open(doc_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            filename = os.path.basename(doc_file)
            base_id = filename.replace('.md', '')
            
            # ë§ˆí¬ë‹¤ìš´ ì²­í‚¹
            chunks = chunk_markdown(content, chunk_size=500, overlap=100)
            print(f"  ğŸ“„ {filename}: {len(content)}ì â†’ {len(chunks)}ê°œ ì²­í¬")
            
            for i, chunk_info in enumerate(chunks):
                chunk_id = f"{base_id}_chunk_{i}"
                chunk_text = chunk_info["text"]
                chunk_section = chunk_info["section"]
                
                documents.append(chunk_text)
                ids.append(chunk_id)
                metadatas.append({
                    "source": filename,
                    "type": "disaster_guide",
                    "section": chunk_section,
                    "chunk_index": i,
                    "chunk_count": len(chunks)
                })
                chunk_counter += 1
                
        except Exception as e:
            print(f"  âœ— {doc_file} ì ì¬ ì‹¤íŒ¨: {e}")
    
    # Chromaì— ë¬¸ì„œ ì¶”ê°€ (Gemini ì„ë² ë”© ìë™ ìƒì„±)
    if documents:
        # ë°°ì¹˜ ì²˜ë¦¬ (í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ì¶”ê°€í•˜ì§€ ì•Šë„ë¡)
        batch_size = 100
        for i in range(0, len(documents), batch_size):
            batch_docs = documents[i:i+batch_size]
            batch_ids = ids[i:i+batch_size]
            batch_metas = metadatas[i:i+batch_size]
            
            chroma_collection.add(
                documents=batch_docs,
                ids=batch_ids,
                metadatas=batch_metas
            )
            print(f"  âœ“ ë°°ì¹˜ {i//batch_size + 1} ì ì¬ ì™„ë£Œ ({len(batch_docs)}ê°œ ì²­í¬)")
        
        print(f"\nâœ“ ì´ {chunk_counter}ê°œ ì²­í¬ë¥¼ Chromaì— ì ì¬í–ˆìŠµë‹ˆë‹¤ (ì›ë³¸ {len(doc_files)}ê°œ íŒŒì¼, Gemini ì„ë² ë”© ì‚¬ìš©).")
    else:
        print("\nâœ— ì ì¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")


# Neo4j ë°ì´í„° ì ì¬ í•¨ìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ)
def load_neo4j_data(nodes_csv: str, relationships_csv: str, session):
    """CSV íŒŒì¼ì—ì„œ Neo4jë¡œ ë°ì´í„°ë¥¼ ì ì¬í•©ë‹ˆë‹¤."""
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    session.run("MATCH (n) DETACH DELETE n")
    print("âœ“ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
    
    # ë…¸ë“œ CSV ì½ê¸°
    nodes_df = pd.read_csv(nodes_csv, encoding='utf-8-sig')
    print(f"\në…¸ë“œ CSV ë¡œë“œ: {len(nodes_df)}ê°œ")
    
    # ë…¸ë“œ íƒ€ì…ë³„ë¡œ ë°°ì¹˜ ì ì¬
    node_types = nodes_df['type'].unique()
    
    for node_type in node_types:
        type_nodes = nodes_df[nodes_df['type'] == node_type]
        print(f"\n{node_type} ë…¸ë“œ ì ì¬ ì¤‘... ({len(type_nodes)}ê°œ)")
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        batch_size = 100
        
        for batch_start in range(0, len(type_nodes), batch_size):
            batch_end = min(batch_start + batch_size, len(type_nodes))
            batch = type_nodes.iloc[batch_start:batch_end]
            
            # ë°°ì¹˜ë³„ ì¿¼ë¦¬ ìƒì„±
            queries = []
            for _, row in batch.iterrows():
                props = {}
                for col in type_nodes.columns:
                    if col not in ['id', 'type'] and pd.notna(row[col]):
                        value = row[col]
                        # ìˆ«ì ë³€í™˜ ì‹œë„
                        try:
                            if isinstance(value, (int, float)) or (isinstance(value, str) and value.replace('.', '').replace('-', '').isdigit()):
                                value = float(value) if '.' in str(value) else int(float(value))
                        except:
                            pass
                        props[col] = value
                
                # CREATE ì¿¼ë¦¬ ì‘ì„±
                prop_strs = [f"{k}: ${k}" for k in props.keys()]
                create_props = ", ".join(["id: $id"] + prop_strs)
                query = f"CREATE (n:{node_type} {{{create_props}}})"
                params = {"id": row['id'], **props}
                queries.append((query, params))
            
            # ë°°ì¹˜ ì‹¤í–‰
            for query, params in queries:
                try:
                    session.run(query, **params)
                except Exception as e:
                    print(f"  âš  ì˜¤ë¥˜: {params.get('id')} - {e}")
            
            if (batch_start // batch_size + 1) % 10 == 0:
                print(f"  ì§„í–‰ ì¤‘: {batch_end}/{len(type_nodes)} ({batch_end*100//len(type_nodes)}%)")
        
        print(f"  âœ“ {node_type} ë…¸ë“œ ì ì¬ ì™„ë£Œ")
    
    # ê´€ê³„ CSV ì½ê¸° ë° ë°°ì¹˜ ì ì¬
    relationships_df = pd.read_csv(relationships_csv, encoding='utf-8-sig')
    print(f"\nê´€ê³„ CSV ë¡œë“œ: {len(relationships_df)}ê°œ")
    
    batch_size = 500
    for batch_start in range(0, len(relationships_df), batch_size):
        batch_end = min(batch_start + batch_size, len(relationships_df))
        batch = relationships_df.iloc[batch_start:batch_end]
        
        for _, rel in batch.iterrows():
            query = f"""
            MATCH (from:{rel['from_type']} {{id: $from_id}})
            MATCH (to:{rel['to_type']} {{id: $to_id}})
            MERGE (from)-[:{rel['relationship_type']}]->(to)
            """
            try:
                session.run(query, from_id=rel['from_id'], to_id=rel['to_id'])
            except Exception as e:
                print(f"  âš  ê´€ê³„ ì˜¤ë¥˜: {rel['from_id']} -> {rel['to_id']} - {e}")
        
        if (batch_start // batch_size + 1) % 5 == 0:
            print(f"  ì§„í–‰ ì¤‘: {batch_end}/{len(relationships_df)} ({batch_end*100//len(relationships_df)}%)")
    
    print(f"  âœ“ ê´€ê³„ ì ì¬ ì™„ë£Œ: {len(relationships_df)}ê°œ")
    
    # ìµœì¢… í†µê³„
    node_count = session.run("MATCH (n) RETURN count(n) as count").single()["count"]
    rel_count = session.run("MATCH ()-[r]->() RETURN count(r) as count").single()["count"]
    print(f"\nâœ“ Neo4j ë°ì´í„° ì ì¬ ì™„ë£Œ: ë…¸ë“œ {node_count}ê°œ, ê´€ê³„ {rel_count}ê°œ")

# ë°ì´í„° ìƒíƒœ í™•ì¸ ë° ì ì¬
with get_neo4j_session() as session:
    # í˜„ì¬ ìƒíƒœ í™•ì¸
    node_count = session.run("MATCH (n) RETURN count(n) as count").single()["count"]
    rel_count = session.run("MATCH ()-[r]->() RETURN count(r) as count").single()["count"]
    
    # ì˜ˆìƒ ë°ì´í„° ê°œìˆ˜ (CSV í™•ì¸) - preprocessing02.ipynb ê²°ê³¼ ì‚¬ìš©
    # preprocessing02.ipynbì—ì„œ ìƒì„±í•œ ì™„ì „í•œ ê·¸ë˜í”„ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
    NODES_CSV = "data/processed/neo4j_nodes_complete.csv"
    REL_CSV = "data/processed/neo4j_relationships_complete.csv"
    
    expected_nodes = 0
    expected_rels = 0
    
    if os.path.exists(NODES_CSV):
        nodes_df = pd.read_csv(NODES_CSV, encoding='utf-8-sig')
        expected_nodes = len(nodes_df)
    
    if os.path.exists(REL_CSV):
        rels_df = pd.read_csv(REL_CSV, encoding='utf-8-sig')
        expected_rels = len(rels_df)
    
    print(f"\n{'='*60}")
    print("Neo4j ë°ì´í„° ìƒíƒœ í™•ì¸")
    print(f"{'='*60}")
    print(f"í˜„ì¬ ë…¸ë“œ: {node_count}ê°œ (ì˜ˆìƒ: {expected_nodes}ê°œ)")
    print(f"í˜„ì¬ ê´€ê³„: {rel_count}ê°œ (ì˜ˆìƒ: {expected_rels}ê°œ)")
    
    # ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¶ˆì™„ì „í•œ ê²½ìš° ì ì¬
    if node_count == 0 or node_count < expected_nodes * 0.9:  # 90% ë¯¸ë§Œì´ë©´ ì¬ì ì¬
        if node_count > 0:
            print(f"\nâš  ë°ì´í„°ê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ì¬ì ì¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
        else:
            print(f"\në°ì´í„° ì ì¬ ì‹œì‘...")
        
        if os.path.exists(NODES_CSV) and os.path.exists(REL_CSV):
            load_neo4j_data(NODES_CSV, REL_CSV, session)
        else:
            print(f"\nâœ— CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
            print(f"  - {NODES_CSV}")
            print(f"  - {REL_CSV}")
            print("  ë¨¼ì € preprocessing02.ipynbë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    else:
        print(f"\nâœ“ Neo4j ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ì ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # ë…¸ë“œ íƒ€ì…ë³„ í†µê³„
        print("\në…¸ë“œ íƒ€ì…ë³„ í†µê³„:")
        for label in session.run("CALL db.labels()").values():
            label_name = label[0]
            count = session.run(f"MATCH (n:{label_name}) RETURN count(n) as count").single()["count"]
            print(f"  - {label_name}: {count}ê°œ")
        
        # ê´€ê³„ í†µê³„ í™•ì¸
        print("\nê´€ê³„ í†µê³„:")
        rel_stats = session.run("""
        MATCH ()-[r]->()
        RETURN type(r) as rel_type, count(r) as count
        ORDER BY count DESC
        """).data()
        for stat in rel_stats:
            print(f"  - {stat['rel_type']}: {stat['count']}ê°œ")
        
        # ì˜ˆì‹œ: ê°•ë‚¨êµ¬ ê´€ê³„ í™•ì¸ (IN ê´€ê³„)
        print("\nê°•ë‚¨êµ¬ ê´€ê³„ í™•ì¸ (ì˜ˆì‹œ):")
        gangnam_test = session.run("""
        MATCH (s:Shelter)-[r:IN]->(a:Admin {gu: 'ê°•ë‚¨êµ¬'})
        RETURN count(s) as shelter_count
        """).single()
        if gangnam_test:
            print(f"  - ê°•ë‚¨êµ¬ Shelter ë…¸ë“œ ìˆ˜ (IN ê´€ê³„): {gangnam_test['shelter_count']}ê°œ")
        else:
            print("  - ê°•ë‚¨êµ¬ ê´€ê³„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# Gemini API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

gemini_client = genai.Client(api_key=GOOGLE_API_KEY)
print("âœ“ Gemini API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

# Gemini ì„ë² ë”© í•¨ìˆ˜ ì •ì˜
class GeminiEmbeddingFunction:
    """Chromaìš© Gemini ì„ë² ë”© í•¨ìˆ˜"""
    
    def __init__(self, client: genai.Client, model: str = "gemini-embedding-001"):
        self.client = client
        self.model = model
    
    def __call__(self, input_texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            # ê¸°ì¡´ Chroma ì»¬ë ‰ì…˜ì´ 384ì°¨ì›ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
            result = self.client.models.embed_content(
                model=self.model,
                contents=input_texts,
                config=types.EmbedContentConfig(
                    output_dimensionality=384  # ê¸°ì¡´ ì»¬ë ‰ì…˜ê³¼ í˜¸í™˜ë˜ë„ë¡ 384ì°¨ì› ì‚¬ìš©
                )
            )
            # embeddings ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            # result.embeddingsëŠ” ì„ë² ë”© ê°ì²´ ë¦¬ìŠ¤íŠ¸
            embeddings = []
            for embedding in result.embeddings:
                # ì„ë² ë”© ê°ì²´ì—ì„œ ë²¡í„° ê°’ ì¶”ì¶œ
                if hasattr(embedding, 'values'):
                    # Embedding ê°ì²´ì¸ ê²½ìš°
                    embeddings.append(list(embedding.values))
                elif isinstance(embedding, list):
                    # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                    embeddings.append(embedding)
                elif hasattr(embedding, '__iter__') and not isinstance(embedding, str):
                    # ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´ì¸ ê²½ìš°
                    embeddings.append(list(embedding))
                else:
                    # ê¸°íƒ€ ê²½ìš° (ë‹¨ì¼ ê°’ ë“±)
                    embeddings.append([float(embedding)])
            return embeddings
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            raise

# Gemini ì„ë² ë”© í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
gemini_embedding_fn = GeminiEmbeddingFunction(gemini_client)
print("âœ“ Gemini ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™” ì™„ë£Œ")


def get_neo4j_schema(session) -> str:
    """Neo4j ê·¸ë˜í”„ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (preprocessing02.ipynbì˜ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ í¬í•¨)."""
    
    # ë…¸ë“œ ë¼ë²¨ ì¡°íšŒ
    node_labels_query = "CALL db.labels()"
    node_labels = [record["label"] for record in session.run(node_labels_query)]
    
    # ê´€ê³„ íƒ€ì… ì¡°íšŒ
    rel_types_query = "CALL db.relationshipTypes()"
    rel_types = [record["relationshipType"] for record in session.run(rel_types_query)]
    
    # ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” ê´€ê³„ íƒ€ì…ë§Œ í•„í„°ë§
    # preprocessing02.ipynbì—ì„œ ìƒì„±í•œ ê´€ê³„ íƒ€ì…ë“¤: IN, GUIDES, TRIGGERS, CAUSES, INCREASES_RISK_OF, UPDATES ë“±
    actual_rels = []
    rel_counts = {}
    for rel_type in rel_types:
        count = session.run(f"MATCH ()-[r:{rel_type}]->() RETURN count(r) as count").single()["count"]
        if count > 0:  # ì‹¤ì œë¡œ ë°ì´í„°ê°€ ìˆëŠ” ê´€ê³„ë§Œ í¬í•¨
            actual_rels.append(rel_type)
            rel_counts[rel_type] = count
    
    # ë…¸ë“œë³„ ì†ì„± ì •ë³´ ì¡°íšŒ
    node_properties = {}
    for label in node_labels:
        props_query = f"""
        MATCH (n:{label})
        RETURN keys(n) as props
        LIMIT 1
        """
        result = session.run(props_query)
        record = result.single()
        if record:
            node_properties[label] = record["props"]
    
    # ìŠ¤í‚¤ë§ˆ ë¬¸ìì—´ ìƒì„±
    schema_parts = ["# Neo4j Graph Schema (Complete from preprocessing02.ipynb)\n\n"]
    
    schema_parts.append("## Node Labels:\n")
    for label in sorted(node_labels):
        props = node_properties.get(label, [])
        # ì£¼ìš” ì†ì„±ë§Œ í‘œì‹œ (ë„ˆë¬´ ê¸¸ì§€ ì•Šë„ë¡)
        key_props = [p for p in props if p not in ['id', 'type']][:10]
        props_str = ", ".join(key_props) if key_props else "id, type"
        schema_parts.append(f"- {label}: {{id, type, {props_str}...}}")
    
    schema_parts.append("\n## Relationship Types:\n")
    # preprocessing02.ipynbì—ì„œ ìƒì„±í•œ ê´€ê³„ íƒ€ì…ë“¤ì„ í¬í•¨í•˜ì—¬ í‘œì‹œ
    # ì¤‘ìš” ê´€ê³„ íƒ€ì… ìš°ì„  í‘œì‹œ
    important_rels = ['GUIDES', 'TRIGGERS', 'CAUSES', 'INCREASES_RISK_OF', 'UPDATES', 'IN']
    for rel_type in important_rels:
        if rel_type in actual_rels:
            schema_parts.append(f"- {rel_type} ({rel_counts[rel_type]}ê°œ)")
    
    # ë‚˜ë¨¸ì§€ ê´€ê³„ íƒ€ì…
    for rel_type in sorted(actual_rels):
        if rel_type not in important_rels:
            schema_parts.append(f"- {rel_type} ({rel_counts[rel_type]}ê°œ)")
    
    return "\n".join(schema_parts)

# ìŠ¤í‚¤ë§ˆ ì •ë³´ í™•ì¸
with get_neo4j_session() as session:
    schema = get_neo4j_schema(session)
    print(schema)


def generate_cypher_query(question: str, schema: str) -> str:
    """Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ ì§ˆë¬¸ì„ Cypher ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (preprocessing02.ipynbì˜ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ì§€ì›)."""
    
    prompt = f"""
ë‹¹ì‹ ì€ Neo4j Cypher ì¿¼ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê·¸ë˜í”„ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ Cypher ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.

# Graph Schema:
{schema}

# ì‚¬ìš© ê°€ëŠ¥í•œ ë…¸ë“œ íƒ€ì…:
- Shelter, TemporaryHousing: ëŒ€í”¼ì†Œ ë° ì„ì‹œì£¼ê±°ì‹œì„¤
- Admin: í–‰ì •êµ¬ì—­ (gu, sigungu ì†ì„±)
- Hazard: ì¬ë‚œ ìœ í˜• (ì§€ì§„, ì‚°ì‚¬íƒœ, ë¶•ê´´, ë…¸í™”)
- Policy: í–‰ë™ìš”ë ¹ ì •ì±…
- Event: ì¬ë‚œ ì´ë²¤íŠ¸

# ì‚¬ìš© ê°€ëŠ¥í•œ ê´€ê³„ íƒ€ì…:
- IN: Shelter/TemporaryHousing â†’ Admin (ìœ„ì¹˜ ê´€ê³„)
- GUIDES: Policy â†’ Hazard (í–‰ë™ìš”ë ¹ì´ ì¬ë‚œì„ ì•ˆë‚´)
- TRIGGERS: Hazard â†’ Hazard ë˜ëŠ” Event â†’ Hazard (ìœ ë°œ ê´€ê³„)
- CAUSES: Hazard â†’ Hazard (ì›ì¸ ê´€ê³„)
- INCREASES_RISK_OF: Hazard â†’ Hazard (ìœ„í—˜ ì¦ê°€ ê´€ê³„)
- UPDATES: Event â†’ Hazard (ì´ë²¤íŠ¸ê°€ ì¬ë‚œ ì •ë³´ ì—…ë°ì´íŠ¸)

# ì˜ˆì‹œ:
# - "ëª‡ ê°œ" ì§ˆë¬¸: MATCH (s:Shelter)-[:IN]->(a:Admin {{gu: 'ê°•ë‚¨êµ¬'}}) RETURN count(s)
# - "ì•Œë ¤ì£¼ì„¸ìš”" ì§ˆë¬¸: MATCH (s:Shelter)-[:IN]->(a:Admin {{gu: 'ì„œì´ˆêµ¬'}}) RETURN s.name, s.address LIMIT 10
# - "í–‰ë™ìš”ë ¹" ì§ˆë¬¸: MATCH (p:Policy)-[:GUIDES]->(h:Hazard {{hazard_type: 'ì§€ì§„'}}) RETURN p.name, p.content LIMIT 5
# - "ì¬ë‚œ ê´€ê³„" ì§ˆë¬¸: MATCH (h1:Hazard)-[:TRIGGERS]->(h2:Hazard) RETURN h1.name, h2.name

# ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ì§€ì¹¨:
1. ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ì ì ˆí•œ Cypher ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
2. ë…¸ë“œ ë¼ë²¨ê³¼ ê´€ê³„ íƒ€ì…ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”. ìœ„ì— ë‚˜ì—´ëœ ê´€ê³„ íƒ€ì…ì„ í™œìš©í•˜ì„¸ìš”.
3. **ì¤‘ìš”**: "ì•Œë ¤ì£¼ì„¸ìš”", "ì°¾ê³  ì‹¶ì–´ìš”", "ì–´ë””" ê°™ì€ ì§ˆë¬¸ì´ë©´ ë°˜ë“œì‹œ ì‹¤ì œ ì •ë³´ë¥¼ ë°˜í™˜í•˜ë„ë¡ RETURN ì ˆì„ ì‘ì„±í•˜ì„¸ìš”.
   - ëŒ€í”¼ì†Œ: MATCH (s:Shelter)-[:IN]->(a:Admin {{gu: 'ì„œì´ˆêµ¬'}}) RETURN s.name, s.address, s.shelter_type LIMIT 10
   - í–‰ë™ìš”ë ¹: MATCH (p:Policy)-[:GUIDES]->(h:Hazard) RETURN p.name, p.content LIMIT 5
   - COUNT()ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì‹¤ì œ ë…¸ë“œ ì •ë³´ë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
4. "ëª‡ ê°œ" ë˜ëŠ” "ê°œìˆ˜" ì§ˆë¬¸ì´ë©´ COUNT()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
5. ì¬ë‚œ ê´€ë ¨ ì§ˆë¬¸ì€ Hazard, Policy, Event ë…¸ë“œë¥¼ í™œìš©í•˜ì„¸ìš”.
6. ì¿¼ë¦¬ë§Œ ë°˜í™˜í•˜ê³  ì„¤ëª…ì€ ì œì™¸í•˜ì„¸ìš”.

Cypher ì¿¼ë¦¬:
"""

    try:
        response = gemini_client.models.generate_content(
            model='gemini-2.5-flash',
            config=types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(
                    include_thoughts=False
                )
            ),
            contents=prompt.strip()
        )
        cypher_query = response.text.strip()
        # ì½”ë“œ ë¸”ë¡ ì œê±° (```cypher ë“±)
        if cypher_query.startswith("```"):
            lines = cypher_query.split("\n")
            cypher_query = "\n".join(lines[1:-1]) if len(lines) > 2 else cypher_query
        
        return cypher_query
    except Exception as e:
        print(f"Cypher ì¿¼ë¦¬ ìƒì„± ì˜¤ë¥˜: {e}")
        return None


def graph_rag_search(question: str, schema: str, session) -> Dict:
    """Graph RAG ê²€ìƒ‰: Neo4jì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    
    # 1. Cypher ì¿¼ë¦¬ ìƒì„±
    cypher_query = generate_cypher_query(question, schema)
    
    if not cypher_query:
        return {"query": None, "results": [], "count": 0, "error": "Cypher ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨"}
    
    print(f"\n[ìƒì„±ëœ Cypher ì¿¼ë¦¬]\n{cypher_query}\n")
    
    # 2. ì¿¼ë¦¬ ì‹¤í–‰
    try:
        result = session.run(cypher_query)
        records = [dict(record) for record in result]
        
        # COUNT ë“±ì˜ ì§‘ê³„ í•¨ìˆ˜ ê²°ê³¼ ì²˜ë¦¬
        # ì§‘ê³„ ì¿¼ë¦¬ëŠ” ë‹¨ì¼ ë ˆì½”ë“œë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, ê²°ê³¼ ê°’ ìì²´ë¥¼ ì¹´ìš´íŠ¸ë¡œ ì‚¬ìš©
        actual_count = len(records)
        if actual_count == 1 and records:
            # COUNT ì¿¼ë¦¬ ê²°ê³¼ ì¶”ì¶œ ì‹œë„
            for key, value in records[0].items():
                if 'count' in key.lower() or isinstance(value, (int, float)):
                    actual_count = int(value) if isinstance(value, (int, float)) else actual_count
        
        return {
            "query": cypher_query,
            "results": records,
            "count": actual_count
        }
    except Exception as e:
        print(f"  âš  Cypher ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return {
            "query": cypher_query,
            "results": [],
            "count": 0,
            "error": str(e)
        }


def vector_rag_search(question: str, collection, client: genai.Client, top_k: int = 5) -> Dict:
    """Vector RAG ê²€ìƒ‰: Chromaì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤ (Gemini ì„ë² ë”© ì‚¬ìš©)."""
    
    if collection is None:
        return {"results": [], "error": "Chroma ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."}
    
    try:
        # Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        try:
            embedding_result = client.models.embed_content(
                model="gemini-embedding-001",
                contents=[question],
                config=types.EmbedContentConfig(
                    output_dimensionality=384  # ê¸°ì¡´ ì»¬ë ‰ì…˜ê³¼ í˜¸í™˜ë˜ë„ë¡ 384ì°¨ì› ì‚¬ìš©
                )
            )
            
            # ì„ë² ë”© ë²¡í„° ì¶”ì¶œ (ì˜ˆì œ ì½”ë“œ ì°¸ê³ )
            query_embedding = None
            if embedding_result.embeddings:
                embedding = embedding_result.embeddings[0]
                # ì„ë² ë”© ê°ì²´ì—ì„œ ë²¡í„° ê°’ ì¶”ì¶œ
                if hasattr(embedding, 'values'):
                    query_embedding = list(embedding.values)
                elif isinstance(embedding, list):
                    query_embedding = embedding
                elif hasattr(embedding, '__iter__') and not isinstance(embedding, str):
                    query_embedding = list(embedding)
                else:
                    query_embedding = [float(embedding)]
            
            if query_embedding is None:
                raise ValueError("ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨")
            
            # Chromaì—ì„œ ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k
            )
        except Exception as embed_error:
            # Gemini ì„ë² ë”© ì‹¤íŒ¨ ì‹œ, Chromaì˜ ê¸°ë³¸ ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš© (í…ìŠ¤íŠ¸ ê¸°ë°˜)
            print(f"  âš  Gemini ì„ë² ë”© ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜: {embed_error}")
            results = collection.query(
                query_texts=[question],
                n_results=top_k
            )
        
        documents = []
        for i in range(len(results["ids"][0])):
            doc_id = results["ids"][0][i]
            doc_text = results["documents"][0][i] if results["documents"] else ""
            distance = results["distances"][0][i] if results["distances"] else None
            
            documents.append({
                "id": doc_id,
                "text": doc_text,
                "distance": distance
            })
        
        return {
            "results": documents,
            "count": len(documents)
        }
    except Exception as e:
        return {
            "results": [],
            "error": str(e)
        }


def format_graph_results(graph_results: Dict, max_length: int = 2000) -> str:
    """Graph RAG ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    
    if graph_results.get("error"):
        return f"ì˜¤ë¥˜: {graph_results['error']}"
    
    result_count = graph_results.get('count', 0)
    result_parts = ["# Graph RAG ê²€ìƒ‰ ê²°ê³¼\n"]
    result_parts.append(f"Cypher ì¿¼ë¦¬: {graph_results.get('query', 'N/A')}\n")
    result_parts.append(f"ê²°ê³¼ ê°œìˆ˜: {result_count}\n")
    
    if not graph_results.get("results"):
        result_parts.append("\nê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        results = graph_results["results"]
        # ê²°ê³¼ê°€ ë§ìœ¼ë©´ ìš”ì•½
        if len(results) > 20:
            result_parts.append(f"\n## ìš”ì•½: ì´ {len(results)}ê°œ ê²°ê³¼ ì¤‘ ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ\n")
            results = results[:10]
        
        for i, record in enumerate(results, 1):
            result_parts.append(f"\n## ê²°ê³¼ {i}")
            for key, value in record.items():
                # ê°’ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                value_str = str(value)
                if len(value_str) > 200:
                    value_str = value_str[:200] + "..."
                result_parts.append(f"- {key}: {value_str}")
    
    text = "\n".join(result_parts)
    # ì „ì²´ ê¸¸ì´ ì œí•œ
    if len(text) > max_length:
        text = text[:max_length] + "\n\n[ì´í•˜ ìƒëµ - ê²°ê³¼ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤]"
    
    return text


def format_vector_results(vector_results: Dict, max_length: int = 3000) -> str:
    """Vector RAG ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    
    if vector_results.get("error"):
        return f"ì˜¤ë¥˜: {vector_results['error']}"
    
    if not vector_results.get("results"):
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    result_parts = ["# Vector RAG ê²€ìƒ‰ ê²°ê³¼\n"]
    result_parts.append(f"ê²°ê³¼ ê°œìˆ˜: {vector_results.get('count', 0)}\n")
    
    for i, doc in enumerate(vector_results["results"], 1):
        result_parts.append(f"\n## ë¬¸ì„œ {i} (ê±°ë¦¬: {doc.get('distance', 'N/A')})")
        doc_text = doc.get('text', '')
        # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        if len(doc_text) > 800:
            doc_text = doc_text[:800] + "..."
        result_parts.append(doc_text)
    
    text = "\n".join(result_parts)
    # ì „ì²´ ê¸¸ì´ ì œí•œ
    if len(text) > max_length:
        text = text[:max_length] + "\n\n[ì´í•˜ ìƒëµ]"
    
    return text


def extract_shelter_info(graph_results: Dict, question: str = "") -> str:
    """Graph RAG ê²°ê³¼ì—ì„œ ëŒ€í”¼ì†Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ í˜•ì‹í™”ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    if not graph_results.get("results"):
        return None
    
    results = graph_results["results"]
    count = graph_results.get("count", 0)
    
    # Shelter ë…¸ë“œ ì •ë³´ ì¶”ì¶œ
    shelters = []
    
    for record in results:
        # recordëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœ: {'s': <Node ...>} ë˜ëŠ” {'name': '...', 'address': '...'} ë“±
        for key, value in record.items():
            # Neo4j Node ê°ì²´ì¸ ê²½ìš° (properties ì†ì„±ì´ ìˆìŒ)
            if hasattr(value, 'properties'):
                try:
                    props = value.properties
                    # Shelter ë…¸ë“œì¸ì§€ í™•ì¸ (labels í™•ì¸)
                    is_shelter = False
                    if hasattr(value, '_labels'):
                        labels = value._labels
                        is_shelter = 'Shelter' in labels or isinstance(labels, frozenset) and 'Shelter' in labels
                    elif hasattr(value, 'labels'):
                        labels = value.labels
                        is_shelter = 'Shelter' in labels
                    else:
                        # labels ì†ì„±ì´ ì—†ìœ¼ë©´ ë¬¸ìì—´ë¡œ í™•ì¸
                        value_str = str(value)
                        is_shelter = 'Shelter' in value_str or 'shelter' in key.lower()
                    
                    if is_shelter or 'name' in props or 'address' in props:  # Shelter ì†ì„±ì´ ìˆìœ¼ë©´ ì¶”ê°€
                        shelter_name = props.get('name', 'ì´ë¦„ ì—†ìŒ')
                        shelter_address = props.get('address', 'ì£¼ì†Œ ì—†ìŒ')
                        shelter_type = props.get('shelter_type', '')
                        # ì‹¤ì œ ê°’ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                        if shelter_name != 'ì´ë¦„ ì—†ìŒ' or shelter_address != 'ì£¼ì†Œ ì—†ìŒ':
                            shelters.append({
                                'name': shelter_name,
                                'address': shelter_address,
                                'type': shelter_type
                            })
                except Exception as e:
                    # properties ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
                    pass
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (ì§ì ‘ ì†ì„±ì´ ìˆëŠ” ê²½ìš° ë˜ëŠ” dict(record)ë¡œ ë³€í™˜ëœ ê²½ìš°)
            elif isinstance(value, dict):
                if 'name' in value or 'address' in value or 'shelter_type' in value:
                    shelters.append({
                        'name': value.get('name', value.get('s.name', 'ì´ë¦„ ì—†ìŒ')),
                        'address': value.get('address', value.get('s.address', 'ì£¼ì†Œ ì—†ìŒ')),
                        'type': value.get('shelter_type', value.get('s.shelter_type', ''))
                    })
        # record ìì²´ê°€ ì†ì„± ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (RETURN s.name, s.address ê°™ì€ ì¿¼ë¦¬)
        # ëª¨ë“  ê°’ì„ í™•ì¸í–ˆëŠ”ë°ë„ sheltersì— ì¶”ê°€ë˜ì§€ ì•Šì€ ê²½ìš°
        if isinstance(record, dict):
            # recordì˜ ì§ì ‘ ì†ì„± í™•ì¸ (RETURN s.name ê°™ì€ ê²½ìš°)
            if 'name' in record or 'address' in record:
                record_name = record.get('name', record.get('s.name', ''))
                record_address = record.get('address', record.get('s.address', ''))
                if record_name or record_address:
                    # ì¤‘ë³µ ì²´í¬
                    is_duplicate = any(
                        s.get('name') == record_name and s.get('address') == record_address 
                        for s in shelters
                    )
                    if not is_duplicate:
                        shelters.append({
                            'name': record_name if record_name else 'ì´ë¦„ ì—†ìŒ',
                            'address': record_address if record_address else 'ì£¼ì†Œ ì—†ìŒ',
                            'type': record.get('shelter_type', record.get('s.shelter_type', ''))
                        })
    
    # Shelter ì •ë³´ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
    if not shelters:
        # ë””ë²„ê¹…: ê²°ê³¼ êµ¬ì¡° í™•ì¸
        if results:
            print(f"  âš  ëŒ€í”¼ì†Œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: ê²°ê³¼ {len(results)}ê°œ, ì²« ë²ˆì§¸ ê²°ê³¼ íƒ€ì…: {type(results[0])}")
            if isinstance(results[0], dict):
                print(f"    ì²« ë²ˆì§¸ ê²°ê³¼ í‚¤: {list(results[0].keys())}")
                for key, value in list(results[0].items())[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                    print(f"    - {key}: {type(value)}")
        return None
    
    # ì¤‘ë³µ ì œê±° (nameê³¼ addressê°€ ê°™ì€ ê²½ìš°)
    unique_shelters = []
    seen = set()
    for shelter in shelters:
        # 'ì´ë¦„ ì—†ìŒ'ì´ë‚˜ 'ì£¼ì†Œ ì—†ìŒ'ì€ ì œì™¸
        if shelter['name'] != 'ì´ë¦„ ì—†ìŒ' and shelter['address'] != 'ì£¼ì†Œ ì—†ìŒ':
            key = (shelter['name'], shelter['address'])
            if key not in seen:
                seen.add(key)
                unique_shelters.append(shelter)
    
    # ì¤‘ë³µ ì œê±° í›„ì—ë„ ì—†ìœ¼ë©´ None
    if not unique_shelters:
        return None
    
    # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
    display_shelters = unique_shelters[:10]
    remaining_count = len(unique_shelters) - len(display_shelters)
    
    # ì§ˆë¬¸ì—ì„œ êµ¬ ì´ë¦„ ì¶”ì¶œ
    import re
    gu_match = re.search(r'(ê°•ë‚¨êµ¬|ì„œì´ˆêµ¬|ì†¡íŒŒêµ¬|ê°•ë™êµ¬|ê´‘ì§„êµ¬|ì„±ë™êµ¬|ì¤‘ë‘êµ¬|ë™ëŒ€ë¬¸êµ¬|ì„±ë¶êµ¬|ê°•ë¶êµ¬|ë…¸ì›êµ¬|ë„ë´‰êµ¬|ì€í‰êµ¬|ì„œëŒ€ë¬¸êµ¬|ë§ˆí¬êµ¬|ìš©ì‚°êµ¬|ì¢…ë¡œêµ¬|ì¤‘êµ¬|ì˜ë“±í¬êµ¬|ì–‘ì²œêµ¬|ê°•ì„œêµ¬|êµ¬ë¡œêµ¬|ê¸ˆì²œêµ¬|ê´€ì•…êµ¬)', question)
    gu_name = gu_match.group(1) if gu_match else ""
    
    # ë‹µë³€ ìƒì„±
    answer_parts = []
    
    if count > 0:
        if gu_name:
            answer_parts.append(f"{gu_name}ì— ì´ {count}ê°œì˜ ëŒ€í”¼ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
        else:
            answer_parts.append(f"ì´ {count}ê°œì˜ ëŒ€í”¼ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
    elif len(unique_shelters) > 0:
        if gu_name:
            answer_parts.append(f"{gu_name}ì— ì´ {len(unique_shelters)}ê°œì˜ ëŒ€í”¼ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
        else:
            answer_parts.append(f"ì´ {len(unique_shelters)}ê°œì˜ ëŒ€í”¼ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
    
    if display_shelters:
        answer_parts.append("ì£¼ìš” ëŒ€í”¼ì†Œ ëª©ë¡:")
        for i, shelter in enumerate(display_shelters, 1):
            shelter_type = shelter['type'] if shelter['type'] else 'ëŒ€í”¼ì†Œ'
            answer_parts.append(f"{i}. {shelter['name']} ({shelter_type})")
            if shelter['address'] and shelter['address'] != 'ì£¼ì†Œ ì—†ìŒ':
                answer_parts.append(f"   ì£¼ì†Œ: {shelter['address']}")
        
        if remaining_count > 0:
            answer_parts.append(f"\nì´ ì™¸ì— {remaining_count}ê°œì˜ ëŒ€í”¼ì†Œê°€ ë” ìˆìŠµë‹ˆë‹¤.")
    
    return "\n".join(answer_parts) if answer_parts else None


def hybrid_rag(question: str, schema: str, neo4j_session, chroma_collection) -> Dict:
    """Hybrid RAG: Graph RAGì™€ Vector RAGë¥¼ ê²°í•©í•˜ì—¬ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # 1. Graph RAG ê²€ìƒ‰
    print("=" * 60)
    print("[Graph RAG ê²€ìƒ‰ ì¤‘...]")
    graph_results = graph_rag_search(question, schema, neo4j_session)
    
    # 2. Vector RAG ê²€ìƒ‰
    print("\n" + "=" * 60)
    print("[Vector RAG ê²€ìƒ‰ ì¤‘...]")
    vector_results = vector_rag_search(question, chroma_collection, gemini_client)
    
    # 3. ê²°ê³¼ í¬ë§·íŒ… (ê¸¸ì´ ì œí•œ)
    graph_text = format_graph_results(graph_results, max_length=2000)
    vector_text = format_vector_results(vector_results, max_length=3000)
    
    # 4. Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
    print("\n" + "=" * 60)
    print("[Ollamaë¥¼ í†µí•œ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...]")
    print("  í”„ë¡¬í”„íŠ¸ ê¸¸ì´:", len(f"{question}\n{graph_text}\n{vector_text}"), "ì")
    
    # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ ì§€ì¹¨ ì œê³µ (preprocessing02.ipynbì˜ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ í™œìš©)
    if 'ì•Œë ¤ì£¼ì„¸ìš”' in question or 'ì°¾ê³  ì‹¶ì–´ìš”' in question or 'ì–´ë””' in question:
        instructions = """- Graph RAG ê²°ê³¼ì— ì‹¤ì œ ëŒ€í”¼ì†Œ/ì‹œì„¤ ëª©ë¡ì´ ìˆìœ¼ë©´ êµ¬ì²´ì ì¸ ì´ë¦„ê³¼ ì£¼ì†Œë¥¼ ì œì‹œí•˜ì„¸ìš”
- ëŒ€í”¼ì†Œê°€ ë§ìœ¼ë©´ ì£¼ìš” ëŒ€í”¼ì†Œ ëª‡ ê°œë¥¼ ì˜ˆì‹œë¡œ ì œì‹œí•˜ê³ , ì´ ê°œìˆ˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”
- ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ ì œê³µ"""
    elif 'í–‰ë™' in question or 'ì–´ë–»ê²Œ' in question or 'ìš”ë ¹' in question:
        instructions = """- Graph RAG ê²°ê³¼ì— Policy(í–‰ë™ìš”ë ¹) ì •ë³´ê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”
- Vector RAG ê²°ê³¼ì˜ í–‰ë™ìš”ë ¹ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ì¸ í–‰ë™ ì§€ì¹¨ì„ ì œê³µí•˜ì„¸ìš”
- ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ ì œê³µ"""
    elif 'ì¬ë‚œ' in question or 'ìœ„í—˜' in question or 'Hazard' in question or 'ì§€ì§„' in question or 'ì‚°ì‚¬íƒœ' in question:
        instructions = """- Graph RAG ê²°ê³¼ì—ì„œ Hazard ë…¸ë“œì™€ ê´€ë ¨ ê´€ê³„(TRIGGERS, CAUSES, INCREASES_RISK_OF)ë¥¼ í™œìš©í•˜ì„¸ìš”
- ì¬ë‚œ ê°„ì˜ ì—°ì‡„ ê´€ê³„ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ë°ì´í„°ì™€ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”"""
    else:
        instructions = """- êµ¬ì²´ì ì¸ ë°ì´í„°(ê°œìˆ˜, ìœ„ì¹˜ ë“±)ë¥¼ ëª…í™•íˆ ì œì‹œ
- Graph RAGì™€ Vector RAG ê²°ê³¼ë¥¼ ëª¨ë‘ í™œìš©í•˜ì„¸ìš”
- ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ ì œê³µ
- ê·¼ê±°ë¥¼ ê°„ë‹¨íˆ ëª…ì‹œ"""
    
    final_prompt = f"""ì¬ë‚œëŒ€ì‘ ì „ë¬¸ê°€ë¡œì„œ ì•„ë˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

[Graph RAG ê²°ê³¼]
{graph_text}

[Vector RAG ê²°ê³¼]  
{vector_text}

ì§€ì¹¨:
{instructions}

ë‹µë³€:"""
    
    try:
        # "ì•Œë ¤ì£¼ì„¸ìš”", "ì°¾ê³  ì‹¶ì–´ìš”" ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•´ ë¨¼ì € extract_shelter_infoë¡œ êµ¬ì²´ì ì¸ ë‹µë³€ ìƒì„±
        graph_count = graph_results.get('count', 0)
        is_list_question = 'ì•Œë ¤ì£¼ì„¸ìš”' in question or 'ì°¾ê³  ì‹¶ì–´ìš”' in question or ('ì–´ë””' in question and graph_count > 0)
        
        extracted_info = None
        if is_list_question and graph_count > 0:
            # Graph RAG ê²°ê³¼ì—ì„œ ëŒ€í”¼ì†Œ ì •ë³´ ì¶”ì¶œ
            extracted_info = extract_shelter_info(graph_results, question)
            if extracted_info:
                print(f"  âœ“ ëŒ€í”¼ì†Œ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_info)}ì")
        
        final_answer = gemini_client.models.generate_content(
            model='gemini-2.5-flash',
            config=types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(
                    include_thoughts=False
                )
            ),
            contents=final_prompt.strip()
        )
        
        # Ollama ì‘ë‹µ ê²€ì¦: êµ¬ì²´ì ì¸ ëŒ€í”¼ì†Œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if final_answer and is_list_question and extracted_info:
            # Ollama ë‹µë³€ì— êµ¬ì²´ì ì¸ ëŒ€í”¼ì†Œ ì´ë¦„ì´ë‚˜ ì£¼ì†Œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            has_specific_info = any(keyword in final_answer for keyword in ['ì£¼ì†Œ:', 'ìœ„ì¹˜:', 'ëª…', 'ê³³', 'ì„œìš¸', 'êµ¬'])
            # Ollama ë‹µë³€ì´ ì¼ë°˜ì ì¸ ì„¤ëª…ë§Œ í¬í•¨í•˜ê³  ìˆìœ¼ë©´ ì¶”ì¶œëœ ì •ë³´ ì‚¬ìš©
            if not has_specific_info or len(final_answer) < 100:
                print(f"  âš  Ollama ë‹µë³€ì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì¶”ì¶œëœ ì •ë³´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                final_answer = extracted_info
        
        # Ollama ì‘ë‹µì´ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° fallback ë‹µë³€ ìƒì„±
        if not final_answer or (isinstance(final_answer, str) and final_answer.strip() == ""):
            print("  âš  Ollama ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: ì‘ë‹µì´ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            # Fallback: ì¶”ì¶œëœ ì •ë³´ ë˜ëŠ” Graph RAG ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            
            # ì§ˆë¬¸ì—ì„œ êµ¬ ì´ë¦„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
            import re
            gu_match = re.search(r'(ê°•ë‚¨êµ¬|ì„œì´ˆêµ¬|ì†¡íŒŒêµ¬|ê°•ë™êµ¬|ê´‘ì§„êµ¬|ì„±ë™êµ¬|ì¤‘ë‘êµ¬|ë™ëŒ€ë¬¸êµ¬|ì„±ë¶êµ¬|ê°•ë¶êµ¬|ë…¸ì›êµ¬|ë„ë´‰êµ¬|ì€í‰êµ¬|ì„œëŒ€ë¬¸êµ¬|ë§ˆí¬êµ¬|ìš©ì‚°êµ¬|ì¢…ë¡œêµ¬|ì¤‘êµ¬|ì˜ë“±í¬êµ¬|ì–‘ì²œêµ¬|ê°•ì„œêµ¬|êµ¬ë¡œêµ¬|ê¸ˆì²œêµ¬|ê´€ì•…êµ¬)', question)
            gu_name = gu_match.group(1) if gu_match else ""
            
            # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ë‹µë³€ ìƒì„±
            if 'ëª‡ ê°œ' in question or 'ê°œìˆ˜' in question or 'count' in question.lower():
                if graph_count > 0:
                    if gu_name:
                        final_answer = f"{gu_name}ì— ìˆëŠ” ëŒ€í”¼ì†ŒëŠ” {graph_count}ê°œì…ë‹ˆë‹¤."
                    else:
                        final_answer = f"ê²€ìƒ‰ ê²°ê³¼: {graph_count}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                else:
                    final_answer = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            elif 'ì–´ë””' in question or 'ìœ„ì¹˜' in question or 'ì•Œë ¤ì£¼ì„¸ìš”' in question or 'ì°¾ê³  ì‹¶ì–´ìš”' in question:
                if extracted_info:
                    final_answer = extracted_info
                elif graph_count > 0:
                    final_answer = f"{gu_name if gu_name else 'í•´ë‹¹ ì§€ì—­'}ì— ìˆëŠ” ëŒ€í”¼ì†ŒëŠ” ì´ {graph_count}ê°œì…ë‹ˆë‹¤. ìƒì„¸ ì •ë³´ëŠ” Graph RAG ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
                else:
                    final_answer = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            elif 'ì–´ë–»ê²Œ' in question or 'í–‰ë™' in question:
                # Vector RAG ê²°ê³¼ë¥¼ í™œìš©í•œ ë‹µë³€
                vector_count = vector_results.get('count', 0)
                if vector_count > 0:
                    final_answer = "Vector RAG ê²€ìƒ‰ ê²°ê³¼ì— ê´€ë ¨ í–‰ë™ ìš”ë ¹ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
                else:
                    final_answer = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                if graph_count > 0:
                    final_answer = f"ê²€ìƒ‰ ê²°ê³¼ë¥¼ {graph_count}ê°œ ì°¾ì•˜ìŠµë‹ˆë‹¤. Graph RAG ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
                else:
                    final_answer = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            print(f"  âœ“ Fallback ë‹µë³€ ìƒì„±: {final_answer[:50]}...")
        
        return {
            "question": question,
            "graph_results": graph_results,
            "vector_results": vector_results,
            "answer": final_answer
        }
    except Exception as e:
        print(f"  âš  ìµœì¢… ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        # Fallback ë‹µë³€ ìƒì„±
        graph_count = graph_results.get('count', 0)
        
        # ì§ˆë¬¸ì—ì„œ êµ¬ ì´ë¦„ ì¶”ì¶œ
        import re
        gu_match = re.search(r'(ê°•ë‚¨êµ¬|ì„œì´ˆêµ¬|ì†¡íŒŒêµ¬|ê°•ë™êµ¬|ê´‘ì§„êµ¬|ì„±ë™êµ¬|ì¤‘ë‘êµ¬|ë™ëŒ€ë¬¸êµ¬|ì„±ë¶êµ¬|ê°•ë¶êµ¬|ë…¸ì›êµ¬|ë„ë´‰êµ¬|ì€í‰êµ¬|ì„œëŒ€ë¬¸êµ¬|ë§ˆí¬êµ¬|ìš©ì‚°êµ¬|ì¢…ë¡œêµ¬|ì¤‘êµ¬|ì˜ë“±í¬êµ¬|ì–‘ì²œêµ¬|ê°•ì„œêµ¬|êµ¬ë¡œêµ¬|ê¸ˆì²œêµ¬|ê´€ì•…êµ¬)', question)
        gu_name = gu_match.group(1) if gu_match else "í•´ë‹¹ êµ¬"
        
        if 'ëª‡ ê°œ' in question or 'ê°œìˆ˜' in question:
            if graph_count > 0:
                fallback_answer = f"{gu_name}ì— ìˆëŠ” ëŒ€í”¼ì†ŒëŠ” {graph_count}ê°œì…ë‹ˆë‹¤. (ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ê°„ë‹¨ ë‹µë³€)"
            else:
                fallback_answer = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}. Graph RAG ê²°ê³¼: {graph_count}ê°œ"
        else:
            if graph_count > 0:
                fallback_answer = f"ê²€ìƒ‰ ê²°ê³¼ë¥¼ {graph_count}ê°œ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ê°„ë‹¨ ë‹µë³€)"
            else:
                fallback_answer = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}. Graph RAG ê²°ê³¼: {graph_count}ê°œ"
        
        return {
            "question": question,
            "graph_results": graph_results,
            "vector_results": vector_results,
            "answer": fallback_answer,
            "error": str(e)
        }


# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import os
import json
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# Neo4j
from neo4j import GraphDatabase

# ChromaDB
import chromadb
from chromadb.config import Settings

# Gemini API
from google import genai

# ê¸°íƒ€
import pandas as pd

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")

# Neo4j ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸°
with get_neo4j_session() as session:
    schema = get_neo4j_schema(session)
    
    # ì‹œë¯¼ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì§ˆì˜ (preprocessing02.ipynbì˜ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆë¥¼ í™œìš©)
    test_questions = [
        "ê°•ë‚¨êµ¬ì— ìˆëŠ” ëŒ€í”¼ì†ŒëŠ” ëª‡ ê°œì¸ê°€ìš”?",
        "ì§€ì§„ì´ ë°œìƒí–ˆì„ ë•Œ ê°€ê¹Œìš´ ëŒ€í”¼ì†Œë¥¼ ì°¾ê³  ì‹¶ì–´ìš”. ì„œì´ˆêµ¬ ê·¼ì²˜ ëŒ€í”¼ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ì„ì‹œì£¼ê±°ì‹œì„¤ì€ ì–´ë””ì— ìˆë‚˜ìš”?",
        "ì§€ì§„ ë°œìƒ ì‹œ ì–´ë–»ê²Œ í–‰ë™í•´ì•¼ í•˜ë‚˜ìš”?",
        "ê³µìŠµê²½ë³´ê°€ ë°œë ¹ë˜ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
        "ì„œì´ˆêµ¬ì˜ ì˜¥ì™¸ëŒ€í”¼ì†Œ ì¤‘ ê°€ì¥ í° ì‹œì„¤ì€ ì–´ë””ì¸ê°€ìš”?",
        "ì§€ì§„ì´ ë‹¤ë¥¸ ì¬ë‚œì„ ìœ ë°œí•  ìˆ˜ ìˆë‚˜ìš”?",
        "ì‚°ì‚¬íƒœ ìœ„í—˜ ì§€ì—­ ê·¼ì²˜ì— ëŒ€í”¼ì†Œê°€ ìˆë‚˜ìš”?",
        "ë…¸í›„ ì‹œì„¤ë¬¼ì´ ë¶•ê´´ ìœ„í—˜ì„ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆë‚˜ìš”?"
    ]
    
    # ì²« ë²ˆì§¸ ì§ˆì˜ í…ŒìŠ¤íŠ¸
    question = test_questions[0]
    print(f"\n{'='*60}")
    print(f"ì§ˆë¬¸: {question}")
    print(f"{'='*60}\n")
    
    result = hybrid_rag(
        question=question,
        schema=schema,
        neo4j_session=session,
        chroma_collection=chroma_collection
    )
    
    print("\n" + "=" * 60)
    print("=" * 60)
    print(result.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"))


# Neo4j ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸°
with get_neo4j_session() as session:
    schema = get_neo4j_schema(session)
    
    # ì‹œë¯¼ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì§ˆì˜ (preprocessing02.ipynbì˜ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆë¥¼ í™œìš©)
    test_questions = [
        "2025-11-02 15:00 ì„œìš¸íŠ¹ë³„ì‹œ ë™ë‚¨ìª½ 2km ì§€ì—­ M6.3 ì§€ì§„ / ë‚™í•˜ë¬¼, ì—¬ì§„ì£¼ì˜  êµ­ë¯¼ì¬ë‚œì•ˆì „í¬í„¸ ì°¸ê³  ëŒ€ì‘",
        "ì§€ì§„ì´ ë°œìƒí–ˆì„ ë•Œ ê°€ê¹Œìš´ ëŒ€í”¼ì†Œë¥¼ ì°¾ê³  ì‹¶ì–´ìš”. ì„œì´ˆêµ¬ ê·¼ì²˜ ëŒ€í”¼ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ì§€ì§„ ë°œìƒ ì‹œ ì–´ë–»ê²Œ í–‰ë™í•´ì•¼ í•˜ë‚˜ìš”?",
        "ë…¸í›„ ì‹œì„¤ë¬¼ì´ ë¶•ê´´ ìœ„í—˜ì„ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆë‚˜ìš”?"
    ]
    
    # ì²« ë²ˆì§¸ ì§ˆì˜ í…ŒìŠ¤íŠ¸
    question = test_questions[0]
    print(f"\n{'='*60}")
    print(f"ì§ˆë¬¸: {question}")
    print(f"{'='*60}\n")
    
    result = hybrid_rag(
        question=question,
        schema=schema,
        neo4j_session=session,
        chroma_collection=chroma_collection
    )
    
    print("\n" + "=" * 60)
    print("=" * 60)
    print(result.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"))


# ì—¬ëŸ¬ ì§ˆì˜ ì¼ê´„ ì‹¤í–‰
with get_neo4j_session() as session:
    schema = get_neo4j_schema(session)
    
    results = []
    for i, question in enumerate(test_questions, 1):
        print(f"\n\n{'#'*60}")
        print(f"ì§ˆì˜ {i}/{len(test_questions)}: {question}")
        print(f"{'#'*60}\n")
        
        result = hybrid_rag(
            question=question,
            schema=schema,
            neo4j_session=session,
            chroma_collection=chroma_collection
        )
        
        results.append(result)
        
        print(f"\n{'='*60}")
        print("[ìµœì¢… ë‹µë³€]")
        print(f"{'='*60}")
        print(result)
        print(result.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"))
        print(f"\n{'='*60}\n")


# ê²°ê³¼ ìš”ì•½
print("\n" + "="*60)
print("# Hybrid RAG ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
print("="*60 + "\n")

for i, result in enumerate(results, 1):
    question = result.get("question", "N/A")
    graph_count = result.get("graph_results", {}).get("count", 0)
    vector_count = result.get("vector_results", {}).get("count", 0)
    has_answer = result.get("answer") is not None
    
    print(f"ì§ˆì˜ {i}: {question}")
    print(f"  - Graph RAG: {graph_count}ê°œ ê²°ê³¼")
    print(f"  - Vector RAG: {vector_count}ê°œ ê²°ê³¼")
    print(f"  - ìµœì¢… ë‹µë³€: {'âœ“ ìƒì„±ë¨' if has_answer else 'âœ— ìƒì„± ì‹¤íŒ¨'}")
    print(result.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"))
    print()


# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import os
import json
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# Neo4j
from neo4j import GraphDatabase

# ChromaDB
import chromadb
from chromadb.config import Settings

# Gemini API
from google import genai

# ê¸°íƒ€
import pandas as pd

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")

# Neo4j ì—°ê²° ì¢…ë£Œ
neo4j_driver.close()
print("âœ“ Neo4j ì—°ê²° ì¢…ë£Œ")
